{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Environment settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/michaelfi/anaconda3/envs/tf/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from matplotlib.pyplot import imshow\n",
    "from matplotlib import pyplot as plot\n",
    "import cv2 as cv\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_PATH = 'data/train/'\n",
    "TEST_PATH = 'data/test/'\n",
    "NUM_OF_CLASSES = 28\n",
    "mean_image = 14.114832\n",
    "std_image = 29.73986"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.10.0\n"
     ]
    }
   ],
   "source": [
    "# Set tf basic settings\n",
    "print(tf.__version__)\n",
    "tf.logging.set_verbosity(tf.logging.INFO)\n",
    "tf.enable_eager_execution()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset manipulation and handling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature encoding for creating tfrecords "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _int64_feature(value):\n",
    "    return tf.train.Feature(int64_list=tf.train.Int64List(value=value))\n",
    "def _bytes_feature(value):\n",
    "    return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function for loading an image from a given file name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image_specific_filter(file):\n",
    "    img = cv.imread(file)\n",
    "    img = img.astype(np.float32)\n",
    "    return img[:,:,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creation of 2 tfrecords files, 1 for training set and 1 for validation set\n",
    "This should only be used once as the tfrecords created are what will be used later on during test time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = pd.read_csv(TRAIN_PATH + \"train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reached image 0 out of 24857\n",
      "Reached image 1000 out of 24857\n",
      "Reached image 2000 out of 24857\n",
      "Reached image 3000 out of 24857\n",
      "Reached image 4000 out of 24857\n",
      "Reached image 5000 out of 24857\n",
      "Reached image 6000 out of 24857\n",
      "Reached image 7000 out of 24857\n",
      "Reached image 8000 out of 24857\n",
      "Reached image 9000 out of 24857\n",
      "Reached image 10000 out of 24857\n",
      "Reached image 11000 out of 24857\n",
      "Reached image 12000 out of 24857\n",
      "Reached image 13000 out of 24857\n",
      "Reached image 14000 out of 24857\n",
      "Reached image 15000 out of 24857\n",
      "Reached image 16000 out of 24857\n",
      "Reached image 17000 out of 24857\n",
      "Reached image 18000 out of 24857\n",
      "Reached image 19000 out of 24857\n",
      "Reached image 20000 out of 24857\n",
      "Reached image 21000 out of 24857\n",
      "Reached image 22000 out of 24857\n",
      "Reached image 23000 out of 24857\n",
      "Reached image 24000 out of 24857\n"
     ]
    }
   ],
   "source": [
    "train_filename = 'data/tfrec/train.tfrecords_full'\n",
    "writer = tf.python_io.TFRecordWriter(train_filename)\n",
    "num_of_examples = train_labels.count()['Id']\n",
    "num_train_set = int(num_of_examples * 0.8)\n",
    "num_validation_set = num_of_examples - num_train_set\n",
    "entire_train_ds = np.zeros((1, 512, 512), dtype=np.float32)\n",
    "\n",
    "for i in range(0, int(num_train_set)):                        \n",
    "    # Create image and labels\n",
    "    if (i % 1000 == 0):\n",
    "        print(\"Reached image {} out of {}\".format(i, num_train_set))\n",
    "    file_name = (TRAIN_PATH + train_labels['Id'][i] + '_green.png')\n",
    "    img = load_image_specific_filter(file_name)\n",
    "    labels = np.zeros(shape=(NUM_OF_CLASSES), dtype=int)\n",
    "    train_label = [int(s) for s in train_labels['Target'][i].split(' ')]\n",
    "    labels[train_label] = 1\n",
    "    \n",
    "    # Create a feature\n",
    "    feature = {'train/label': _int64_feature(labels),\n",
    "               'train/image': _bytes_feature(tf.compat.as_bytes(img.tostring()))}\n",
    "    # Create an example protocol buffer\n",
    "    example = tf.train.Example(features=tf.train.Features(feature=feature))\n",
    "    \n",
    "    # Serialize to string and write on the file\n",
    "    writer.write(example.SerializeToString())\n",
    "    \n",
    "    # We will save all of the dataset info to create a mean and std to be used during the preprocessing\n",
    "    if (i < 1500):\n",
    "        img = np.expand_dims(img, axis=0)\n",
    "        entire_train_ds = np.append(entire_train_ds, img, axis=0)\n",
    "    \n",
    "    \n",
    "writer.close()   \n",
    "mean_image = np.mean(entire_train_ds)\n",
    "std_image = np.std(entire_train_ds)\n",
    "del(entire_train_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reached image 25000 out of 31072\n",
      "Reached image 26000 out of 31072\n",
      "Reached image 27000 out of 31072\n",
      "Reached image 28000 out of 31072\n",
      "Reached image 29000 out of 31072\n",
      "Reached image 30000 out of 31072\n",
      "Reached image 31000 out of 31072\n"
     ]
    }
   ],
   "source": [
    "test_filename = 'data/tfrec/train.tfrecords_test'\n",
    "writer = tf.python_io.TFRecordWriter(test_filename)\n",
    "num_validation_set = num_of_examples - num_train_set\n",
    "\n",
    "for i in range(int(num_train_set), num_of_examples):                        \n",
    "    # Create image and labels\n",
    "    if (i % 1000 == 0):\n",
    "        print(\"Reached image {} out of {}\".format(i, num_of_examples))\n",
    "    file_name = (TRAIN_PATH + train_labels['Id'][i] + '_green.png')\n",
    "    img = load_image_specific_filter(file_name)\n",
    "    labels = np.zeros(shape=(NUM_OF_CLASSES), dtype=int)\n",
    "    train_label = [int(s) for s in train_labels['Target'][i].split(' ')]\n",
    "    labels[train_label] = 1\n",
    "    \n",
    "    # Create a feature\n",
    "    feature = {'test/label': _int64_feature(labels),\n",
    "               'test/image': _bytes_feature(tf.compat.as_bytes(img.tostring()))}\n",
    "    # Create an example protocol buffer\n",
    "    example = tf.train.Example(features=tf.train.Features(feature=feature))\n",
    "    \n",
    "    # Serialize to string and write on the file\n",
    "    writer.write(example.SerializeToString())\n",
    "    \n",
    "    \n",
    "writer.close()   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function for extracting features from tfrecords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_fn_train(data_record):\n",
    "    features = {\n",
    "        # Extract features using the keys set during creation\n",
    "        'train/label': tf.FixedLenFeature((28), tf.int64),\n",
    "        'train/image': tf.FixedLenFeature([], tf.string)\n",
    "    }    \n",
    "    \n",
    "    sample = tf.parse_single_example(data_record, features)\n",
    "    sample['train/image'] = tf.decode_raw(sample['train/image'], tf.float32)\n",
    "    sample['train/image'] = tf.reshape(sample['train/image'], (512,512))\n",
    "    sample['train/image'] = (sample['train/image'] - mean_image)/std_image\n",
    "    \n",
    "    return sample\n",
    "\n",
    "def extract_fn_test(data_record):\n",
    "    features = {\n",
    "        # Extract features using the keys set during creation\n",
    "        'test/label': tf.FixedLenFeature((28), tf.int64),\n",
    "        'test/image': tf.FixedLenFeature([], tf.string)\n",
    "    }    \n",
    "    \n",
    "    sample = tf.parse_single_example(data_record, features)\n",
    "    sample['test/image'] = tf.decode_raw(sample['test/image'], tf.float32)\n",
    "    sample['test/image'] = tf.reshape(sample['test/image'], (512,512))\n",
    "    sample['test/image'] = (sample['test/image'] - mean_image)/std_image\n",
    "\n",
    "    return sample\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating 2 dataset objects, 1 for training set and 1 for validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = tf.data.TFRecordDataset(['data/tfrec/train.tfrecords_test'])\n",
    "test_dataset = test_dataset.map(extract_fn_test)\n",
    "\n",
    "num_epochs = 3\n",
    "batch_size = 50\n",
    "buffer_size = 1000\n",
    "\n",
    "train_dataset = tf.data.TFRecordDataset(['data/tfrec/train.tfrecords_full'])\n",
    "train_dataset = train_dataset.map(extract_fn_train)\n",
    "train_dataset = train_dataset.apply(tf.contrib.data.shuffle_and_repeat(buffer_size, num_epochs))\n",
    "train_dataset = train_dataset.batch(batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProteinClassifier(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Block1\n",
    "        self.block1_conv1 = tf.keras.layers.Conv2D(filters=8,\n",
    "                                                kernel_size=[3, 3],\n",
    "                                                strides=(1, 1),\n",
    "                                                padding='same',\n",
    "                                                activation=tf.nn.leaky_relu,\n",
    "                                                use_bias=True,\n",
    "                                                kernel_initializer=tf.contrib.layers.xavier_initializer_conv2d()\n",
    "                                               )\n",
    "        \n",
    "        self.block1_conv2 = tf.keras.layers.Conv2D(filters=8,\n",
    "                                                kernel_size=[3, 3],\n",
    "                                                strides=(1, 1),\n",
    "                                                padding='same',\n",
    "                                                activation=tf.nn.leaky_relu,\n",
    "                                                use_bias=True,\n",
    "                                                kernel_initializer=tf.contrib.layers.xavier_initializer_conv2d()\n",
    "                                               )\n",
    "        \n",
    "        self.block1_pool = tf.keras.layers.MaxPool2D(pool_size=(2, 2),\n",
    "                                               strides=(2, 2),\n",
    "                                               padding='valid'\n",
    "                                              )\n",
    "        \n",
    "        # Block2\n",
    "        self.block2_conv1 = tf.keras.layers.Conv2D(filters=16,\n",
    "                                                kernel_size=[3, 3],\n",
    "                                                strides=(1, 1),\n",
    "                                                padding='same',\n",
    "                                                activation=tf.nn.leaky_relu,\n",
    "                                                use_bias=True,\n",
    "                                                kernel_initializer=tf.contrib.layers.xavier_initializer_conv2d()\n",
    "                                               )\n",
    "        \n",
    "        self.block2_conv2 = tf.keras.layers.Conv2D(filters=16,\n",
    "                                                kernel_size=[3, 3],\n",
    "                                                strides=(1, 1),\n",
    "                                                padding='same',\n",
    "                                                activation=tf.nn.leaky_relu,\n",
    "                                                use_bias=True,\n",
    "                                                kernel_initializer=tf.contrib.layers.xavier_initializer_conv2d()\n",
    "                                               )\n",
    "        \n",
    "        self.block2_pool = tf.keras.layers.MaxPool2D(pool_size=(2, 2),\n",
    "                                               strides=(2, 2),\n",
    "                                               padding='valid'\n",
    "                                              )\n",
    "        \n",
    "        # Block3\n",
    "        self.block3_conv1 = tf.keras.layers.Conv2D(filters=24,\n",
    "                                                kernel_size=[3, 3],\n",
    "                                                strides=(1, 1),\n",
    "                                                padding='same',\n",
    "                                                activation=tf.nn.leaky_relu,\n",
    "                                                use_bias=True,\n",
    "                                                kernel_initializer=tf.contrib.layers.xavier_initializer_conv2d()\n",
    "                                               )\n",
    "        \n",
    "        self.block3_conv2 = tf.keras.layers.Conv2D(filters=24,\n",
    "                                                kernel_size=[3, 3],\n",
    "                                                strides=(1, 1),\n",
    "                                                padding='same',\n",
    "                                                activation=tf.nn.leaky_relu,\n",
    "                                                use_bias=True,\n",
    "                                                kernel_initializer=tf.contrib.layers.xavier_initializer_conv2d()\n",
    "                                               )\n",
    "        \n",
    "        self.block3_conv3 = tf.keras.layers.Conv2D(filters=24,\n",
    "                                                kernel_size=[3, 3],\n",
    "                                                strides=(1, 1),\n",
    "                                                padding='same',\n",
    "                                                activation=tf.nn.leaky_relu,\n",
    "                                                use_bias=True,\n",
    "                                                kernel_initializer=tf.contrib.layers.xavier_initializer_conv2d()\n",
    "                                               )\n",
    "        \n",
    "        self.block3_pool = tf.keras.layers.MaxPool2D(pool_size=(2, 2),\n",
    "                                               strides=(2, 2),\n",
    "                                               padding='valid'\n",
    "                                              )\n",
    "        \n",
    "        # Block4\n",
    "        self.block4_conv1 = tf.keras.layers.Conv2D(filters=32,\n",
    "                                                kernel_size=[3, 3],\n",
    "                                                strides=(1, 1),\n",
    "                                                padding='same',\n",
    "                                                activation=tf.nn.leaky_relu,\n",
    "                                                use_bias=True,\n",
    "                                                kernel_initializer=tf.contrib.layers.xavier_initializer_conv2d()\n",
    "                                               )\n",
    "        \n",
    "        self.block4_conv2 = tf.keras.layers.Conv2D(filters=32,\n",
    "                                                kernel_size=[3, 3],\n",
    "                                                strides=(1, 1),\n",
    "                                                padding='same',\n",
    "                                                activation=tf.nn.leaky_relu,\n",
    "                                                use_bias=True,\n",
    "                                                kernel_initializer=tf.contrib.layers.xavier_initializer_conv2d()\n",
    "                                               )\n",
    "        \n",
    "        self.block4_conv3 = tf.keras.layers.Conv2D(filters=32,\n",
    "                                                kernel_size=[3, 3],\n",
    "                                                strides=(1, 1),\n",
    "                                                padding='same',\n",
    "                                                activation=tf.nn.leaky_relu,\n",
    "                                                use_bias=True,\n",
    "                                                kernel_initializer=tf.contrib.layers.xavier_initializer_conv2d()\n",
    "                                               )\n",
    "        \n",
    "        self.block4_pool = tf.keras.layers.MaxPool2D(pool_size=(2, 2),\n",
    "                                               strides=(2, 2),\n",
    "                                               padding='valid'\n",
    "                                              )\n",
    "        \n",
    "        # Block4\n",
    "        self.block5_conv1 = tf.keras.layers.Conv2D(filters=64,\n",
    "                                                kernel_size=[3, 3],\n",
    "                                                strides=(1, 1),\n",
    "                                                padding='same',\n",
    "                                                activation=tf.nn.leaky_relu,\n",
    "                                                use_bias=True,\n",
    "                                                kernel_initializer=tf.contrib.layers.xavier_initializer_conv2d()\n",
    "                                               )\n",
    "        \n",
    "        self.block5_conv2 = tf.keras.layers.Conv2D(filters=64,\n",
    "                                                kernel_size=[3, 3],\n",
    "                                                strides=(1, 1),\n",
    "                                                padding='same',\n",
    "                                                activation=tf.nn.leaky_relu,\n",
    "                                                use_bias=True,\n",
    "                                                kernel_initializer=tf.contrib.layers.xavier_initializer_conv2d()\n",
    "                                               )\n",
    "        \n",
    "        self.block5_conv3 = tf.keras.layers.Conv2D(filters=64,\n",
    "                                                kernel_size=[3, 3],\n",
    "                                                strides=(1, 1),\n",
    "                                                padding='same',\n",
    "                                                activation=tf.nn.leaky_relu,\n",
    "                                                use_bias=True,\n",
    "                                                kernel_initializer=tf.contrib.layers.xavier_initializer_conv2d()\n",
    "                                               )\n",
    "        \n",
    "        self.block5_pool = tf.keras.layers.MaxPool2D(pool_size=(2, 2),\n",
    "                                               strides=(2, 2),\n",
    "                                               padding='valid'\n",
    "                                              )\n",
    "        \n",
    "        \n",
    "        \n",
    "        # Dense output layer\n",
    "        self.fc1 = tf.keras.layers.Dense(4096, activation=tf.nn.relu)\n",
    "        \n",
    "        # Dense layer for classes\n",
    "        self.fc2 = tf.keras.layers.Dense(28)\n",
    "        \n",
    "        # Optimizer\n",
    "        self.optimizer = tf.train.AdamOptimizer(learning_rate = 1e-4)\n",
    "        \n",
    "    def call(self, inputs, training=True, **kwargs):\n",
    "        \n",
    "        # Input Layer\n",
    "        input_layer = tf.reshape(inputs, [-1, 512, 512, 1])\n",
    "        \n",
    "        # Block1\n",
    "        x_1 = self.block1_conv1(input_layer)\n",
    "        x_1 = self.block1_conv2(x_1)\n",
    "        x_1 = self.block1_pool(x_1)\n",
    "        \n",
    "        # Block2\n",
    "        x_2 = self.block2_conv1(x_1)\n",
    "        x_2 = self.block2_conv2(x_2)\n",
    "        x_2 = self.block2_pool(x_2)\n",
    "        \n",
    "        # Block3\n",
    "        x_3 = self.block3_conv1(x_2)\n",
    "        x_3 = self.block3_conv2(x_3)\n",
    "        x_3 = self.block3_conv3(x_3)\n",
    "        x_3 = self.block3_pool(x_3)\n",
    "        \n",
    "        # Block4\n",
    "        x_4 = self.block4_conv1(x_3)\n",
    "        x_4 = self.block4_conv2(x_4)\n",
    "        x_4 = self.block4_conv3(x_4)\n",
    "        x_4 = self.block4_pool(x_4)\n",
    "        \n",
    "        # Block3\n",
    "        x_5 = self.block5_conv1(x_4)\n",
    "        x_5 = self.block5_conv2(x_5)\n",
    "        x_5 = self.block5_conv3(x_5)\n",
    "        x_5 = self.block3_pool(x_5)       \n",
    "        x_5 = tf.reshape(x_5, [x_5.shape[0], -1])\n",
    "        \n",
    "        x_fc_1 = self.fc1(x_5)\n",
    "        logits = self.fc2(x_fc_1)\n",
    "        \n",
    "        return logits \n",
    "    \n",
    "    def loss(self, logits, labels):        \n",
    "        \n",
    "        loss = tf.losses.sigmoid_cross_entropy(labels, logits)\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    def optimize(self, inputs, labels, training = True):\n",
    "        with tf.GradientTape(persistent=False) as tape:\n",
    "            logits = self(inputs)\n",
    "            loss = self.loss(logits, labels)\n",
    "        \n",
    "        if  training:\n",
    "            gradients = tape.gradient(loss, self.variables)\n",
    "            self.optimizer.apply_gradients(zip(gradients, self.variables))\n",
    "        del(tape)\n",
    "        return loss\n",
    "    \n",
    "    def test(self, inputs, labels):\n",
    "        logits = self(inputs, training=False)\n",
    "        \n",
    "        # TODO: add the accuracy check\n",
    "        accuracy = 0\n",
    "\n",
    "        return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = []\n",
    "with tf.device(\"/device:CPU:0\"):\n",
    "    model = ProteinClassifier()\n",
    "    itr = train_dataset.make_one_shot_iterator()\n",
    "    for x in itr:\n",
    "        loss = model.optimize(x['train/image'], x['train/label'])\n",
    "        print(loss)\n",
    "        break\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
